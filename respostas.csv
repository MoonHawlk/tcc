pergunta,resposta
O que é processamento de linguagem natural (PLN)?,"O processamento de Linguagem Natural (PLN) é uma subárea da ciência da computação, inteligência artificial e da linguística que se dedica a estudar os problemas da geração e compreensão automática de línguas humanas naturais. Ele converte informações de bancos de dados de computadores em linguagem compreensível ao ser humano (geração de língua natural) e converte ocorrências de língua humana em representações mais formais, mais facilmente manipuláveis por programas de computador (compreensão de língua natural). Um dos desafios do PLN é a aprendizagem de Linguagem Natural (NLL), que é um subcampo dedicado às abordagens de aprendizado na área do PLN. A conferência CoNLL e o SIGNLL, organizações centrais neste campo, são patrocinados pela ACL, reconhecendo também as suas ligações com a linguística computacional e aquisição de língua. Embora o aumento correspondente no número de horas trabalhadas por humanos seja necessário em sistemas de aprendizado de máquina, isso não requer aumentos significativos na complexidade do processo de anotação. (Fonte: Livro de estilo)

Este artigo carece de reciclagem de acordo com o livro de estilo. Se quiser melhorar a qualidade deste artigo, sinta-se livre para editá-lo(a). (Fonte: Dezembro de 2016)

Esta página cita fontes, mas que não cobrem todo o conteúdo. Ajude a inserir referências (Encontre fontes: Google (notícias • livros • acadêmico • imagens livres • WP refs)  • ABW • CAPES). (Fonte: Dezembro de 2016)"
Quais disciplinas o PLN combina?,"Based on the additional context provided, it becomes clear that the PLN combines computer science and linguistics in its applications for evaluating natural language processing tools and speech technologies for EVALITA's Italian website. This is evident from the mention of PLN and voice evaluation tools specifically for the Italian site, as well as the distinction between intrinsic and extrinsic evaluation methods. Intrinsic evaluation focuses on assessing the performance of an isolated PNL system against a predefined standard of excellence, while extrinsic evaluation, also known as usage evaluation, considers the system in a more complex scenario, such as when it is embedded or serves a specific function for human users. Therefore, it can be confidently stated that the PLN combines computer science and linguistics in its applications for evaluating natural language processing tools and speech technologies."
Quais são alguns desafios do PLN?,"Some challenges of NLP (Natural Language Processing) that have emerged in recent years include the need for large annotated datasets to train machine learning models, as well as the limitations and challenges associated with different types of algorithms used in NLP tasks. While some algorithms, such as decision trees, can produce rigid rule sets similar to those used in earlier NLP systems, they may be sensitive to the quality and size of the training data, and may have difficulty handling ambiguity and context. Additionally, NLP models can be computationally expensive to train and run, requiring significant resources in terms of computing power and memory. These challenges highlight the importance of ongoing research and development in the field of NLP to address these issues and improve the accuracy and efficiency of NLP systems."
Quais foram dois sistemas de PLN bem-sucedidos desenvolvidos na década de 1960?,"Entre os dois sistemas de PLN bem-sucedidos desenvolvidos na década de 1960, estão ELIZA e DENDRAL. O texto não fornece mais informações específicas sobre esses sistemas, mas podemos destacar alguns aspectos deles.

ELIZA foi um dos primeiros programas de linguagem natural (PLN) desenvolvidos na década de 1960, criado por Joseph Weizenbaum da MIT. ELIZA era capaz de simular uma conversa com um usuário, respondendo a perguntas e interagindo em linguagem natural. Embora o programa não tivesse conhecimento real do mundo exterior, ele foi capaz de simular respostas inteligentes e criativas, baseadas nas regras programadas por Weizenbaum.

Já DENDRAL foi um sistema de PLN desenvolvido na Yale University pela equipe liderada por Lotfi Zadeh. O sistema era capaz de analisar dados científicos e identificar padrões e relacionamentos entre eles, auxiliando os cientistas a interpretar e compreender os dados em um contexto mais amplo. DENDRAL foi um dos primeiros exemplos de sistemas estatísticos para PLN, que usavam pesos reais para cada característica de entrada, permitindo decisões flexíveis e probabilísticas baseadas nesses dados.

Apesar de ambos os sistemas terem sido desenvolvidos na década de 1960, eles ainda são considerados bem-sucedidos devido à sua influência significativa na área da inteligência artificial e linguagem natural. Eles foram pioneiros em suas respectivas áreas e abriram caminho para o desenvolvimento de sistemas mais complexos e avançados nos anos seguintes.

No contexto atual, que enfatiza a capacidade dos sistemas de lidar com entradas não familiares e errôneas, ambos os sistemas apresentam desafios significativos. ELIZA foi capaz de simular respostas inteligentes e criativas baseadas nas regras programadas por Weizenbaum, mas essas regras eram estáticas e não podiam lidar com entradas desconhecidas ou errôneas. Já DENDRAL usava pesos reais para cada característica de entrada, permitindo decisões flexíveis e probabilísticas baseadas nesses dados, mas ainda era capaz apenas de lidar com entradas conhecidas.

Sistemas baseados em aprender automaticamente as regras podem ser mais precisos simplesmente porque são capazes de lidar com entradas não familiares e errôneas. Esses sistemas aprendem a identificar padrões e relacionamentos entre os dados, sem a necessidade de regras estáticas ou pesos reais para cada característica de entrada. Eles podem lidar com entradas desconhecidas ou errôneas, pois aprendem a adaptar-se às novas situações e ajustar suas decisões conforme necessário.

Em resumo, embora ELIZA e DENDRAL tenham sido sistemas bem-sucedidos na década de 1960, os sistemas baseados em aprender automaticamente as regras representam um avanço significativo na área da inteligência artificial e linguagem natural. Eles são capazes de lidar com entradas não familiares e errôneas, o que torna eles mais precisos e flexíveis em uma ampla variedade de aplicações."
O que foi a Experiência de Georgetown de 1954?,"The Experiencia de Georgetown in 1954 was a significant milestone in the history of natural language processing (NLP) and machine translation. It involved a demonstration at Georgetown University of a simple system that could translate short Russian phrases into English using computer programs. This experiment helped to spark interest and funding for further research in NLP, which aims to enable computers to understand and generate human language. The history of NLP can be traced back to the 1950s, when Alan Turing proposed the concept of a test for intelligence in his article ""Computing Machinery and Intelligence"".

In the following decades, many chatbots were developed, such as PARRY, Racter, and Jabberwacky. Until the 1980s, most NLP systems relied on complex sets of manually written rules. However, in the late 1980s, there was a revolution in NLP with the introduction of algorithms based on machine learning techniques, such as MARGIE (SCHANK, 1975), SAM (CULLINGFORD, 1978), PAM (WILENSKY, 1978), TaleSpin (MEEHAN, 1976), QUALM (LEHNERT, 1977), Politics (CARBONELL, 1979) and Plot Units (LEHNERT, 1981). These systems used statistical methods to learn from large amounts of text data and improve their language understanding capabilities.

Some challenges of NLP include natural language understanding and the ability for computers to extract meaning from human language. While significant progress has been made in this field, there is still a long way to go before computers can fully understand and generate human-like language. Nonetheless, the Experiencia de Georgetown in 1954 marked an important milestone in the history of NLP and machine translation, paving the way for further research and development in this exciting field."
Que algorítmos introduziram a revolução no PLN no final dos anos 1980?,"In the late 1980s, there was a revolution in batch processing (PLN) with the introduction of machine learning algorithms. Prior to this, PLN systems were based on manually written rules, but these rules could be very complex and difficult to maintain. Moreover, to make the systems more accurate, it was necessary to provide more input data, which was a much simpler task with machine learning algorithms. These algorithms could analyze large amounts of data and identify patterns and relationships between them, allowing for the creation of more precise and capable models than those based on manually written rules. This revolution in PLN was made possible by advances in computer science, particularly in the areas of artificial intelligence and machine learning.

In 1987, the first evaluation campaign focused on speech technologies (including general topics such as syntax and semantics), comparing rule-based approaches with statistical methods [12]. In 1994, within the framework of the Parseval / GEIG project, grammar-of-phrase structures were compared [Black 1991]. There was also a series of campaigns in the Tipster project focused on tasks such as summarization, translation, and search (Hirschman 1998).

[12] Reference not provided."
O que faz a tarefa de sumarização automática?,"In summary, automatic summarization involves compressing and condensing a text into a shorter and more concise version while preserving its essential meaning. This task is distinguished from other NLP tasks by the volume dedicated to its research, specific definition of the problem, standard metric for evaluation, standard corpora for evaluation, and dedicated competitions for this particular task. Automated evaluations can also be used to evaluate NLP systems without additional disorderly costs, but for many NLP problems, defining a standard of excellence is complex and may be impossible when inter-annotator agreement is insufficient. In such cases, manual evaluation by human judges instructed to estimate the quality of a system or a sample of its output based on a series of criteria is preferred due to their linguistic expertise. However, it should be noted that while human judges are considered more competent in language matters, their evaluations may still be subjective and prone to errors, unlike automated evaluations which can be repeated as often as necessary without additional costs."
O que é o reconhecimento de entidade nomeada (NER)?,"Named Entity Recognition (NER) is a subtask of natural language processing that involves training a model to automatically identify named entities within text, such as names of people, places, and organizations. However, based on the additional context provided, it becomes clear that NER is not being referred to in this case. The discussion here is about the limitations of rule-based systems and the benefits of using machine learning algorithms instead. While NER is a subtask of natural language processing, it involves training a model to automatically identify named entities within text, whereas the current context is focused on the tradeoffs between manually crafting rules versus providing more data for machine learning models to learn from. Therefore, the original answer remains valid in this case.

In summary, NER is not directly related to the discussion about rule-based systems and machine learning algorithms. However, both NER and these other topics fall under the broader field of natural language processing, which aims to enable computers to understand human language and interact with it in a more intelligent way."
O que distingue avaliação intrínseca de avaliação extrínseca em PLN?,"The key difference between intrinsic evaluation and extrinsic evaluation in PLN is the context in which the performance of the PLN system is assessed. Intrinsic evaluation considers only the isolated performance of the PLN system and compares it to a predefined standard of excellence set by the evaluators, while extrinsic evaluation, also known as usage evaluation, analyzes the PLN system in a more complex scenario such as an embedded system or a critical function for human users.

For instance, in intrinsic evaluation, the PLN system is executed with sample data and its performance is measured based on speed, reliability, resource consumption, and result quality. In contrast, extrinsic evaluation integrates the PLN system into a more complex system and evaluates its accuracy in a realistic scenario.

An analogy to understand this difference is that of black-box vs. Glass-box testing. In black-box testing, the PLN system is executed with sample data, and the results are analyzed without considering the context of the system in which it is being used. In glass-box testing, the PLN system is integrated into the entire system, and its accuracy is evaluated in the realistic scenario in which it is being used.

For example, an extrinsic evaluation could involve executing a PLN system with real data and comparing the results to those obtained by another POS system to measure accuracy.

In summary, while intrinsic evaluation focuses on the isolated performance of the PLN system, extrinsic evaluation considers the overall utility of the PLN system in relation to the global task of the entire system or human user."
Quais são duas vantagens de sistemas de PLN baseados em aprendizagem de máquina em relação aos sistemas baseados em regras?,"In the context provided, the existing answer still accurately highlights two advantages of machine learning-based PLN systems over rule-based ones: (1) Automatic identification of common patterns and relationships in data without the need for explicit rule definition by a domain expert, and (2) Robustness to unknown or out-of-scope inputs through the use of statistical inference algorithms. These advantages make machine learning-based PLN systems more efficient and effective for large datasets with atypical values, while rule-based systems may require more manual effort and be less reliable for inputs outside the scope of defined rules."
